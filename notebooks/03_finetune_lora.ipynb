{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed184e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.9.1+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Install & Imports\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76105e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2b67f74d410>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Base Model + Tokenizer\n",
    "BASE_MODEL_NAME = \"Qwen/Qwen1.5-0.5B-Chat\"  \n",
    "DATA_PATH = \"../data/annotated/training_pairs.jsonl\"\n",
    "OUTPUT_DIR = \"models/platelogic_qwen_lora\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3107dd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 300\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Title: granny is licking her chops\\nIngredient...</td>\n",
       "      <td>Season pork chops with salt, pepper, and musta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Title: crock pot chicken bbq\\nIngredients: ['b...</td>\n",
       "      <td>Season chicken breast with cumin, vinegar, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Title: savory smothered pork chops\\nIngredient...</td>\n",
       "      <td>Season pork chops with salt and pepper. Sear i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  Title: granny is licking her chops\\nIngredient...   \n",
       "1  Title: crock pot chicken bbq\\nIngredients: ['b...   \n",
       "2  Title: savory smothered pork chops\\nIngredient...   \n",
       "\n",
       "                                              output  \n",
       "0  Season pork chops with salt, pepper, and musta...  \n",
       "1  Season chicken breast with cumin, vinegar, and...  \n",
       "2  Season pork chops with salt and pepper. Sear i...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load JSONL into a pandas DataFrame\n",
    "records = []\n",
    "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            records.append(json.loads(line))\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "print(\"Total records:\", len(df))\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c4fd6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 2441.67 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"You are a cooking assistant that rewrites recipes to be high-protein and low-carb.\\n\\nInstruction:\\nTitle: sirloin tips with mushrooms\\nIngredients: ['sirloin tip steaks', 'butter', 'vegetable oil', 'fresh mushrooms', 'garlic cloves', 'dijon mustard', 'cornstarch', 'heavy cream', 'beef broth', 'white wine vinegar', 'soy sauce', 'dried parsley']\\n\\nGoal: Rewrite the cooking steps to produce a high-protein, low-carb version while keeping the core flavor.\\nConstraints: avoid added sugars, avoid starchy carbs, prioritize lean protein.\\n\\nWrite the improved version steps.\\n\\nResponse:\\nSeason the main protein and cook until done. Use low-carb sides and avoid added sugars in this sirloin tips with mushrooms recipe.\"}\n",
      "Train size: 270 Val size: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def build_prompt(example):\n",
    "    instr = example[\"input\"]\n",
    "    resp = example[\"output\"]\n",
    "    text = (\n",
    "        \"You are a cooking assistant that rewrites recipes to be high-protein and low-carb.\\n\\n\"\n",
    "        \"Instruction:\\n\"\n",
    "        f\"{instr}\\n\\n\"\n",
    "        \"Response:\\n\"\n",
    "        f\"{resp}\"\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "hf_ds = Dataset.from_pandas(df)\n",
    "hf_ds = hf_ds.map(build_prompt)\n",
    "\n",
    "hf_ds = hf_ds.remove_columns([c for c in hf_ds.column_names if c != \"text\"])\n",
    "hf_ds = hf_ds.shuffle(seed=SEED)\n",
    "\n",
    "# Train/validation split\n",
    "split = hf_ds.train_test_split(test_size=0.1, seed=SEED)\n",
    "train_ds = split[\"train\"]\n",
    "val_ds = split[\"test\"]\n",
    "\n",
    "print(train_ds[0])\n",
    "print(\"Train size:\", len(train_ds), \"Val size:\", len(val_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e13e50a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 270/270 [00:00<00:00, 4265.01 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 2727.77 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(270, 30)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "# Some chat models donâ€™t have pad_token; align it with eos_token if needed\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "MAX_LENGTH = 512  # keep this modest for your laptop\n",
    "\n",
    "def tokenize_function(example):\n",
    "    out = tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    # For simple supervised fine-tuning, labels = input_ids\n",
    "    out[\"labels\"] = out[\"input_ids\"].copy()\n",
    "    return out\n",
    "\n",
    "tokenized_train = train_ds.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_val = val_ds.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "tokenized_train.set_format(type=\"torch\")\n",
    "tokenized_val.set_format(type=\"torch\")\n",
    "\n",
    "len(tokenized_train), len(tokenized_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a13315d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Number of target modules: 168\n",
      "Sample target modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\n",
      "trainable params: 3,784,704 || all params: 467,772,416 || trainable%: 0.8091\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(151936, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2SdpaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): Qwen2RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2816, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2816, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2816, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((1024,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((1024,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((1024,), eps=1e-06)\n",
       "        (rotary_emb): Qwen2RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch.nn as nn\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    ")\n",
    "\n",
    "# ---- choose which modules to apply LoRA to ----\n",
    "# We focus on attention + MLP projections (standard practice).\n",
    "candidate_keywords = [\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "    \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    \"W_pack\"   # for fused qkv, if present\n",
    "]\n",
    "\n",
    "target_modules = [\n",
    "    name.split(\".\")[-1]\n",
    "    for name, module in model.named_modules()\n",
    "    if isinstance(module, nn.Linear)\n",
    "    and any(kw in name for kw in candidate_keywords)\n",
    "]\n",
    "\n",
    "print(\"Number of target modules:\", len(target_modules))\n",
    "print(\"Sample target modules:\", sorted(set(target_modules))[:15])\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=list(set(target_modules)),\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee4363a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fatem\\Projects\\platelogic\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\fatem\\AppData\\Local\\Temp\\ipykernel_22752\\3844717070.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "  0%|          | 0/67 [00:00<?, ?it/s]c:\\Users\\fatem\\Projects\\platelogic\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 15%|â–ˆâ–        | 10/67 [04:49<27:45, 29.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1659, 'grad_norm': 1.666911005973816, 'learning_rate': 0.00018095238095238095, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–‰       | 20/67 [09:49<23:39, 30.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1246, 'grad_norm': 0.6866673231124878, 'learning_rate': 0.00014920634920634923, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 30/67 [14:58<18:54, 30.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5992, 'grad_norm': 0.5002280473709106, 'learning_rate': 0.00011746031746031746, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 40/67 [20:08<13:56, 30.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3949, 'grad_norm': 0.4756954312324524, 'learning_rate': 8.571428571428571e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 50/67 [38:51<20:04, 70.85s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3932, 'grad_norm': 0.43215397000312805, 'learning_rate': 5.396825396825397e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 50/67 [1:15:58<20:04, 70.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3139999806880951, 'eval_runtime': 2226.9294, 'eval_samples_per_second': 0.013, 'eval_steps_per_second': 0.013, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 60/67 [2:04:57<10:23, 89.05s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3839, 'grad_norm': 0.42747262120246887, 'learning_rate': 2.2222222222222223e-05, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [2:08:18<00:00, 114.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 7698.3476, 'train_samples_per_second': 0.035, 'train_steps_per_second': 0.009, 'train_loss': 1.0906155251744967, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=67, training_loss=1.0906155251744967, metrics={'train_runtime': 7698.3476, 'train_samples_per_second': 0.035, 'train_steps_per_second': 0.009, 'total_flos': 257024738721792.0, 'train_loss': 1.0906155251744967, 'epoch': 0.9925925925925926})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 1 if device == \"cpu\" else 4\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=4 if device == \"cpu\" else 1,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    warmup_ratio=0.05,\n",
    "    weight_decay=0.01,\n",
    "    fp16=(device == \"cuda\"),\n",
    "    bf16=False,\n",
    "    report_to=[],\n",
    "    logging_dir=os.path.join(OUTPUT_DIR, \"logs\")\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # You can add perplexity later; for now we donâ€™t overcomplicate it.\n",
    "    return {}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef7f3ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Base model: Qwen/Qwen1.5-0.5B-Chat\n",
      "LoRA adapter dir: c:\\Users\\fatem\\Projects\\platelogic\\models\\platelogic_qwen_lora\\checkpoint-67\n",
      "âœ… Loaded fine-tuned LoRA model.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(\"..\")  # from notebooks â†’ project root\n",
    "\n",
    "BASE_MODEL_NAME = \"Qwen/Qwen1.5-0.5B-Chat\"  # same as training\n",
    "\n",
    "# POINT HERE â¬‡ï¸\n",
    "ADAPTER_DIR = os.path.join(\n",
    "    PROJECT_ROOT,\n",
    "    \"models\",\n",
    "    \"platelogic_qwen_lora\",\n",
    "    \"checkpoint-67\"\n",
    ")\n",
    "\n",
    "print(\"Base model:\", BASE_MODEL_NAME)\n",
    "print(\"LoRA adapter dir:\", ADAPTER_DIR)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, ADAPTER_DIR)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"âœ… Loaded fine-tuned LoRA model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48ddacdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training pairs loaded: 300\n",
      "Columns: ['input', 'output']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# --- Load the training pairs JSONL (this has `input` + `output`) ---\n",
    "pairs_path = \"../data/annotated/training_pairs.jsonl\"\n",
    "pairs = pd.read_json(pairs_path, lines=True)\n",
    "print(\"Training pairs loaded:\", len(pairs))\n",
    "print(\"Columns:\", pairs.columns.tolist())\n",
    "\n",
    "if \"input\" not in pairs.columns:\n",
    "    raise ValueError(\"Expected an 'input' column in training_pairs.jsonl.\")\n",
    "\n",
    "# --- Generation function ---\n",
    "def generate_recipe(prompt, max_new_tokens=200):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb03f253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FULL PROMPT SENT TO MODEL ===\n",
      "\n",
      "You are a cooking assistant that rewrites recipes to be high-protein and low-carb.\n",
      "\n",
      "Instruction:\n",
      "Title: granny is licking her chops\n",
      "Ingredients: ['pork chops', 'mango chutney', 'granny smith apple', 'sharp cheddar cheese']\n",
      "\n",
      "Goal: Rewrite the cooking steps to produce a high-protein, low-carb version while keeping the core flavor.\n",
      "Constraints: avoid added sugars, avoid starchy carbs, prioritize lean protein.\n",
      "\n",
      "Write the improved version steps.\n",
      "\n",
      "Response:\n",
      "\n",
      "\n",
      "=== MODEL OUTPUT ===\n",
      "\n",
      "You are a cooking assistant that rewrites recipes to be high-protein and low-carb.\n",
      "\n",
      "Instruction:\n",
      "Title: granny is licking her chops\n",
      "Ingredients: ['pork chops', 'mango chutney', 'granny smith apple', 'sharp cheddar cheese']\n",
      "\n",
      "Goal: Rewrite the cooking steps to produce a high-protein, low-carb version while keeping the core flavor.\n",
      "Constraints: avoid added sugars, avoid starchy carbs, prioritize lean protein.\n",
      "\n",
      "Write the improved version steps.\n",
      "\n",
      "Response:\n",
      "Season the main protein with salt and pepper. Use a low-carb side dish or cook the chicken until done. Finish by baking the sides to keep the core flavor.\n"
     ]
    }
   ],
   "source": [
    "sample_input = pairs.iloc[0][\"input\"]\n",
    "\n",
    "prompt = (\n",
    "    \"You are a cooking assistant that rewrites recipes to be high-protein and low-carb.\\n\\n\"\n",
    "    \"Instruction:\\n\"\n",
    "    f\"{sample_input}\\n\\n\"\n",
    "    \"Response:\\n\"\n",
    ")\n",
    "\n",
    "print(\"=== FULL PROMPT SENT TO MODEL ===\\n\")\n",
    "print(prompt[:600])\n",
    "\n",
    "print(\"\\n=== MODEL OUTPUT ===\\n\")\n",
    "print(generate_recipe(prompt, max_new_tokens=200))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 467,772,416 || trainable%: 0.0000\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
